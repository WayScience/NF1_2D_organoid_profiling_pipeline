{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Convert SQLite output(s) to parquet files with CytoTable"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import argparse\n",
                "import logging\n",
                "import os\n",
                "import pathlib\n",
                "import uuid\n",
                "\n",
                "import duckdb\n",
                "import pandas as pd\n",
                "import tqdm\n",
                "from arg_parsing_utils import check_for_missing_args, parse_args\n",
                "\n",
                "# cytotable will merge objects from SQLite file into single cells and save as parquet file\n",
                "from cytotable import convert, presets\n",
                "from notebook_init_utils import bandicoot_check, init_notebook\n",
                "from parsl.config import Config\n",
                "from parsl.executors import HighThroughputExecutor\n",
                "\n",
                "# Set the logging level to a higher level to avoid outputting unnecessary errors from config file in convert function\n",
                "logging.getLogger().setLevel(logging.ERROR)\n",
                "root_dir, in_notebook = init_notebook()\n",
                "image_base_dir = bandicoot_check(\n",
                "    pathlib.Path(os.path.expanduser(\"~/mnt/bandicoot\")).resolve(), root_dir\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running in a notebook\n"
                    ]
                }
            ],
            "source": [
                "if not in_notebook:\n",
                "    args_dict = parse_args()\n",
                "    patient = args_dict[\"patient\"]\n",
                "\n",
                "    check_for_missing_args(\n",
                "        patient=patient,\n",
                "    )\n",
                "else:\n",
                "    print(\"Running in a notebook\")\n",
                "    patient = \"NF0014_T1\"\n",
                "    well_fov = \"E7-1\"\n",
                "    clip_limit = 0.01\n",
                "    twoD_method = \"middle_n\"\n",
                "    overwrite = True\n",
                "\n",
                "\n",
                "max_projected_input = pathlib.Path(\n",
                "    f\"../../data/{patient}/2D_analysis/2a.cellprofiler_zmax_proj_output/\"\n",
                ").resolve(strict=True)\n",
                "middle_slice_input = pathlib.Path(\n",
                "    f\"../../data/{patient}/2D_analysis/2b.cellprofiler_middle_slice_output/\"\n",
                ").resolve(strict=True)\n",
                "middle_n_slice_input = pathlib.Path(\n",
                "    f\"../../data/{patient}/2D_analysis/2c.cellprofiler_middle_n_slice_max_proj_output/\"\n",
                ").resolve(strict=True)\n",
                "\n",
                "# directory for processed data\n",
                "output_dir = pathlib.Path(f\"../../data/{patient}/2D_analysis/0.converted/\").resolve()\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "max_projected_sc_output = pathlib.Path(output_dir, \"max_projected_sc.parquet\").resolve()\n",
                "middle_slice_sc_output = pathlib.Path(output_dir, \"middle_slice_sc.parquet\").resolve()\n",
                "middle_n_slice_sc_output = pathlib.Path(\n",
                "    output_dir, \"middle_n_slice_sc.parquet\"\n",
                ").resolve()\n",
                "\n",
                "max_projected_organoid_output = pathlib.Path(\n",
                "    output_dir, \"max_projected_organoid.parquet\"\n",
                ").resolve()\n",
                "middle_slice_organoid_output = pathlib.Path(\n",
                "    output_dir, \"middle_slice_organoid.parquet\"\n",
                ").resolve()\n",
                "middle_n_slice_organoid_output = pathlib.Path(\n",
                "    output_dir, \"middle_n_slice_organoid.parquet\"\n",
                ").resolve()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Set paths and variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# preset configurations based on typical CellProfiler outputs\n",
                "preset = \"cellprofiler_sqlite_pycytominer\"\n",
                "\n",
                "# update preset to include site metadata and cell counts\n",
                "joins = presets.config[\"cellprofiler_sqlite_pycytominer\"][\"CONFIG_JOINS\"].replace(\n",
                "    \"Image_Metadata_Well,\",\n",
                "    \"Image_Metadata_Well, Image_Metadata_Site, Image_Count_Cells,\",\n",
                ")\n",
                "\n",
                "# type of file output from cytotable (currently only parquet)\n",
                "dest_datatype = \"parquet\"\n",
                "\n",
                "\n",
                "well_fov_dict = {}\n",
                "for sqlite_dir in [max_projected_input, middle_slice_input, middle_n_slice_input]:\n",
                "    twoD_type = sqlite_dir.name.split(\"_out\")[0].split(\"cellprofiler_\")[1]\n",
                "    well_fov_dict[twoD_type] = {}\n",
                "    sqlites = list(sqlite_dir.rglob(\"*sqlite\"))\n",
                "    sqlites.sort()  # sort to ensure consistent order\n",
                "    for file_path in sqlites:\n",
                "        well_fov = file_path.parent.stem\n",
                "        well_fov_dict[twoD_type][well_fov] = {\n",
                "            \"image_path\": file_path,\n",
                "            \"output_dir\": output_dir / twoD_type / f\"{well_fov}\",\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Convert SQLite to parquet file(s) for single-cell profiles"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/home/lippincm/4TB_A/NF1_2D_organoid_profiling_pipeline/data/NF0014_T1/2D_analysis/2a.cellprofiler_zmax_proj_output\n",
                        "/home/lippincm/4TB_A/NF1_2D_organoid_profiling_pipeline/data/NF0014_T1/2D_analysis/2b.cellprofiler_middle_slice_output\n",
                        "/home/lippincm/4TB_A/NF1_2D_organoid_profiling_pipeline/data/NF0014_T1/2D_analysis/2c.cellprofiler_middle_n_slice_max_proj_output\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'zmax_proj': {'df_list': []},\n",
                            " 'middle_slice': {'df_list': []},\n",
                            " 'middle_n_slice_max_proj': {'df_list': []}}"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "output_dict_of_dfs = {}\n",
                "for sqlite_dir in [max_projected_input, middle_slice_input, middle_n_slice_input]:\n",
                "    print(sqlite_dir)\n",
                "    output_dict_of_dfs[sqlite_dir.name.split(\"_out\")[0].split(\"cellprofiler_\")[1]] = {\n",
                "        \"df_list\": [],\n",
                "    }\n",
                "output_dict_of_dfs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total = 0\n",
                "errors = 0\n",
                "# loop through the middle and zmax projected sqlite files\n",
                "for featurization_type in well_fov_dict.keys():\n",
                "    for well_fov, file_info in tqdm.tqdm(well_fov_dict[featurization_type].items()):\n",
                "        sqlite_file = file_info[\"image_path\"]\n",
                "        total += 1\n",
                "        # convert the sqlite file to a single cell parquet file\n",
                "        try:\n",
                "            df = convert(\n",
                "                sqlite_file,\n",
                "                preset=preset,\n",
                "                joins=joins,\n",
                "                chunk_size=500,\n",
                "                dest_datatype=dest_datatype,\n",
                "                dest_path=f\"{well_fov_dict[featurization_type][well_fov]['output_dir']}_sc.parquet\",\n",
                "                parsl_config=Config(\n",
                "                    executors=[HighThroughputExecutor()],\n",
                "                    run_dir=f\"cytotable_runinfo/{uuid.uuid4().hex}\",\n",
                "                ),\n",
                "            )\n",
                "            output_dict_of_dfs[featurization_type][\"df_list\"].append(\n",
                "                f\"{well_fov_dict[featurization_type][well_fov]['output_dir']}_sc.parquet\"\n",
                "            )\n",
                "        except Exception as e:\n",
                "            errors += 1\n",
                "            print(f\"Error processing {sqlite_file}: {e}\")\n",
                "            continue\n",
                "print(f\"Total files processed: {total}\")\n",
                "print(f\"Total errors encountered: {errors}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_dict_of_dfs = {\n",
                "    \"middle_slice\": {\n",
                "        \"df_list\": [\n",
                "            x\n",
                "            for x in pathlib.Path(\n",
                "                f\"../../data/{patient}/0.converted/middle_slice/\"\n",
                "            ).rglob(\"*.parquet\")\n",
                "        ]\n",
                "    },\n",
                "    \"zmax_proj\": {\n",
                "        \"df_list\": [\n",
                "            x\n",
                "            for x in pathlib.Path(f\"../../data/{patient}/0.converted/zmax_proj/\").rglob(\n",
                "                \"*.parquet\"\n",
                "            )\n",
                "        ]\n",
                "    },\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read in the dataframes and concatenate them in place\n",
                "for featurization_type in output_dict_of_dfs.keys():\n",
                "    print(\n",
                "        f\"Concatenating {len(output_dict_of_dfs[featurization_type]['df_list'])} dataframes for {featurization_type}\"\n",
                "    )\n",
                "    df_list = [\n",
                "        pd.read_parquet(df) for df in output_dict_of_dfs[featurization_type][\"df_list\"]\n",
                "    ]\n",
                "    print(len(df_list))\n",
                "    output_dict_of_dfs[featurization_type][\"df\"] = pd.concat(df_list, ignore_index=True)\n",
                "    # Define the list of columns to prioritize and prefix\n",
                "    prioritized_columns = [\n",
                "        \"Nuclei_Location_Center_X\",\n",
                "        \"Nuclei_Location_Center_Y\",\n",
                "        \"Cells_Location_Center_X\",\n",
                "        \"Cells_Location_Center_Y\",\n",
                "        \"Image_Count_Cells\",\n",
                "    ]\n",
                "\n",
                "    # If any, drop rows where \"Metadata_ImageNumber\" is NaN (artifact of cytotable)\n",
                "    output_dict_of_dfs[featurization_type][\"df\"] = output_dict_of_dfs[\n",
                "        featurization_type\n",
                "    ][\"df\"].dropna(subset=[\"Metadata_ImageNumber\"])\n",
                "\n",
                "    # Rearrange columns and add \"Metadata\" prefix in one line\n",
                "    output_dict_of_dfs[featurization_type][\"df\"] = output_dict_of_dfs[\n",
                "        featurization_type\n",
                "    ][\"df\"][\n",
                "        prioritized_columns\n",
                "        + [\n",
                "            col\n",
                "            for col in output_dict_of_dfs[featurization_type][\"df\"].columns\n",
                "            if col not in prioritized_columns\n",
                "        ]\n",
                "    ].rename(\n",
                "        columns=lambda col: \"Metadata_\" + col if col in prioritized_columns else col\n",
                "    )\n",
                "    # rename Image_Metadata_Well\n",
                "    output_dict_of_dfs[featurization_type][\"df\"] = output_dict_of_dfs[\n",
                "        featurization_type\n",
                "    ][\"df\"].rename(columns={\"Image_Metadata_Well\": \"Metadata_Well\"})\n",
                "\n",
                "    if featurization_type == \"middle_slice\":\n",
                "        output_dict_of_dfs[featurization_type][\"df\"].to_parquet(\n",
                "            middle_slice_sc_output, index=False\n",
                "        )\n",
                "    elif featurization_type == \"zmax_proj\":\n",
                "        output_dict_of_dfs[featurization_type][\"df\"].to_parquet(\n",
                "            max_projected_sc_output, index=False\n",
                "        )\n",
                "    print(\n",
                "        f\"Saved {featurization_type} data to {output_dict_of_dfs[featurization_type]['df'].shape[0]} rows in {output_dict_of_dfs[featurization_type]['df'].shape[1]} columns\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extract organoid only profiles"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_dict_of_dfs = {}\n",
                "for sqlite_dir in [middle_slice_input, max_projected_input]:\n",
                "    output_dict_of_dfs[sqlite_dir.name.split(\"_out\")[0].split(\"cellprofiler_\")[1]] = {\n",
                "        \"df_list\": [],\n",
                "    }\n",
                "output_dict_of_dfs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "total = 0\n",
                "errors = 0\n",
                "for featurization_type in well_fov_dict.keys():\n",
                "    print(f\"Processing {featurization_type} files\")\n",
                "    for well_fov, file_info in tqdm.tqdm(well_fov_dict[featurization_type].items()):\n",
                "        well = well_fov.split(\"-\")[0]\n",
                "        fov = well_fov.split(\"-\")[1]\n",
                "        sqlite_file = file_info[\"image_path\"]\n",
                "        total += 1\n",
                "        try:\n",
                "            # Create a DuckDB connection\n",
                "            with duckdb.connect(sqlite_file) as con:\n",
                "                # get the organoid table\n",
                "                organoid_table = con.execute(\"SELECT * FROM Per_Organoid\").df()\n",
                "                organoid_table.rename(\n",
                "                    columns={\n",
                "                        \"ImageNumber\": \"Metadata_ImageNumber\",\n",
                "                        \"Organoid_Number_Object_Number\": \"Metadata_Organoid_Number_Object_Number\",\n",
                "                        \"Image_Metadata_Well\": \"Metadata_Well\",\n",
                "                    },\n",
                "                    inplace=True,\n",
                "                )\n",
                "                organoid_table.insert(0, \"Metadata_Well_FOV\", well_fov)\n",
                "                organoid_table.insert(1, \"Metadata_FOV\", fov)\n",
                "                organoid_table.insert(2, \"Metadata_Well\", well)\n",
                "            output_dict_of_dfs[featurization_type][\"df_list\"].append(organoid_table)\n",
                "\n",
                "        except Exception as e:\n",
                "            errors += 1\n",
                "            print(f\"Error processing {sqlite_file}: {e}\")\n",
                "            continue"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read in the dataframes and concatenate them in place\n",
                "for featurization_type in output_dict_of_dfs.keys():\n",
                "    print(\n",
                "        f\"Concatenating {len(output_dict_of_dfs[featurization_type]['df_list'])} dataframes for {featurization_type}\"\n",
                "    )\n",
                "    output_dict_of_dfs[featurization_type][\"df\"] = pd.concat(\n",
                "        output_dict_of_dfs[featurization_type][\"df_list\"], ignore_index=True\n",
                "    )\n",
                "\n",
                "    # If any, drop rows where \"Metadata_ImageNumber\" is NaN (artifact of cytotable)\n",
                "    output_dict_of_dfs[featurization_type][\"df\"] = output_dict_of_dfs[\n",
                "        featurization_type\n",
                "    ][\"df\"].dropna(subset=[\"Metadata_ImageNumber\"])\n",
                "    if featurization_type == \"middle_slice\":\n",
                "        output_dict_of_dfs[featurization_type][\"df\"].to_parquet(\n",
                "            middle_slice_organoid_output, index=False\n",
                "        )\n",
                "    elif featurization_type == \"zmax_proj\":\n",
                "        output_dict_of_dfs[featurization_type][\"df\"].to_parquet(\n",
                "            max_projected_organoid_output, index=False\n",
                "        )\n",
                "    print(\n",
                "        f\"Saved {featurization_type} data to {output_dict_of_dfs[featurization_type]['df'].shape[0]} rows in {output_dict_of_dfs[featurization_type]['df'].shape[1]} columns\"\n",
                "    )"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nf1_image_based_profiling_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
